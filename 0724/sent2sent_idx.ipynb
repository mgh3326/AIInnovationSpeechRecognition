{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"A barber is a person. \n",
    "a barber is good person. \n",
    "a barber is huge person. \n",
    "he Knew A Secret! \n",
    "The Secret He Kept is huge secret. \n",
    "Huge secret. \n",
    "His barber kept his word. \n",
    "a barber kept his word. \n",
    "His barber kept his secret. \n",
    "But keeping and keeping such a huge secret to himself was driving the barber crazy. \n",
    "the barber went up a huge mountain.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표 1: Word Seq --> Word-ID Seq\n",
    "\n",
    "## 세부목표 1: 문장을 토큰화\n",
    "## 세부목표 2: 단어 Set 및 아이디 부여\n",
    "## 세부목표 3: Word Seq --> Word-ID Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'barber', 'is', 'a', 'person.', 'a', 'barber', 'is', 'good', 'person.', 'a', 'barber', 'is', 'huge', 'person.', 'he', 'Knew', 'A', 'Secret!', 'The', 'Secret', 'He', 'Kept', 'is', 'huge', 'secret.', 'Huge', 'secret.', 'His', 'barber', 'kept', 'his', 'word.', 'a', 'barber', 'kept', 'his', 'word.', 'His', 'barber', 'kept', 'his', 'secret.', 'But', 'keeping', 'and', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'the', 'barber', 'crazy.', 'the', 'barber', 'went', 'up', 'a', 'huge', 'mountain.']\n"
     ]
    }
   ],
   "source": [
    "## 세부 목표 1:\n",
    "text_token = text.split()\n",
    "print(text_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_text: ['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n",
      "sentence :  ['A', 'barber', 'is', 'a', 'person', '.']\n",
      "sentence :  ['a', 'barber', 'is', 'good', 'person', '.']\n",
      "sentence :  ['a', 'barber', 'is', 'huge', 'person', '.']\n",
      "sentence :  ['he', 'Knew', 'A', 'Secret', '!']\n",
      "sentence :  ['The', 'Secret', 'He', 'Kept', 'is', 'huge', 'secret', '.']\n",
      "sentence :  ['Huge', 'secret', '.']\n",
      "sentence :  ['His', 'barber', 'kept', 'his', 'word', '.']\n",
      "sentence :  ['a', 'barber', 'kept', 'his', 'word', '.']\n",
      "sentence :  ['His', 'barber', 'kept', 'his', 'secret', '.']\n",
      "sentence :  ['But', 'keeping', 'and', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'the', 'barber', 'crazy', '.']\n",
      "sentence :  ['the', 'barber', 'went', 'up', 'a', 'huge', 'mountain', '.']\n",
      "sentences:  [['a', 'barber', 'is', 'a', 'person', '.'], ['a', 'barber', 'is', 'good', 'person', '.'], ['a', 'barber', 'is', 'huge', 'person', '.'], ['he', 'knew', 'a', 'secret', '!'], ['the', 'secret', 'he', 'kept', 'is', 'huge', 'secret', '.'], ['huge', 'secret', '.'], ['his', 'barber', 'kept', 'his', 'word', '.'], ['a', 'barber', 'kept', 'his', 'word', '.'], ['his', 'barber', 'kept', 'his', 'secret', '.'], ['but', 'keeping', 'and', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'the', 'barber', 'crazy', '.'], ['the', 'barber', 'went', 'up', 'a', 'huge', 'mountain', '.']]\n",
      "vocab:  Counter({'.': 10, 'a': 8, 'barber': 8, 'secret': 6, 'huge': 5, 'his': 5, 'is': 4, 'kept': 4, 'person': 3, 'the': 3, 'he': 2, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, '!': 1, 'but': 1, 'and': 1, 'such': 1, 'to': 1, 'himself': 1, 'was': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'up': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "sent_text=sent_tokenize(text)\n",
    "print(\"sent_text:\", sent_text)\n",
    "\n",
    "\n",
    "\n",
    "vocab=Counter() # 파이썬의 Counter 모듈을 이용하면 단어의 모든 빈도를 쉽게 계산할 수 있습니다.  \n",
    "\n",
    "sentences = []\n",
    "\n",
    "for i in sent_text:\n",
    "    sentence=word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
    "    print(\"sentence : \", sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in sentence: \n",
    "        word=word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
    "        \n",
    "        result.append(word)\n",
    "        # vocab[word] = vocab[word] + 1 #각 단어의 빈도를 Count 합니다.\n",
    "        vocab[word] += 1\n",
    "    sentences.append(result) \n",
    "\n",
    "print(\"sentences: \", sentences)\n",
    "print(\"vocab: \", vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_sorted:  [('.', 10), ('a', 8), ('barber', 8), ('secret', 6), ('huge', 5), ('his', 5), ('is', 4), ('kept', 4), ('person', 3), ('the', 3), ('he', 2), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('!', 1), ('but', 1), ('and', 1), ('such', 1), ('to', 1), ('himself', 1), ('was', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('up', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab_sorted=sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "print(\"vocab_sorted: \", vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'barber': 2, 'secret': 3, 'huge': 4, 'his': 5, 'is': 6, 'kept': 7, 'person': 8, 'the': 9, 'he': 10, 'word': 11, 'keeping': 12, 'good': 13, 'knew': 14, '!': 15, 'but': 16, 'and': 17, 'such': 18, 'to': 19, 'himself': 20, 'was': 21, 'driving': 22, 'crazy': 23, 'went': 24, 'up': 25, 'mountain': 26}\n"
     ]
    }
   ],
   "source": [
    "word_to_index={}\n",
    "\n",
    "for idx, (word, frequency) in enumerate(vocab_sorted):\n",
    "    word_to_index[word]=idx\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent   :  ['a', 'barber', 'is', 'a', 'person', '.']\n",
      "sent_id:  [1, 2, 6, 1, 8, 0]\n",
      "sent   :  ['a', 'barber', 'is', 'good', 'person', '.']\n",
      "sent_id:  [1, 2, 6, 13, 8, 0]\n",
      "sent   :  ['a', 'barber', 'is', 'huge', 'person', '.']\n",
      "sent_id:  [1, 2, 6, 4, 8, 0]\n",
      "sent   :  ['he', 'knew', 'a', 'secret', '!']\n",
      "sent_id:  [10, 14, 1, 3, 15]\n",
      "sent   :  ['the', 'secret', 'he', 'kept', 'is', 'huge', 'secret', '.']\n",
      "sent_id:  [9, 3, 10, 7, 6, 4, 3, 0]\n",
      "sent   :  ['huge', 'secret', '.']\n",
      "sent_id:  [4, 3, 0]\n",
      "sent   :  ['his', 'barber', 'kept', 'his', 'word', '.']\n",
      "sent_id:  [5, 2, 7, 5, 11, 0]\n",
      "sent   :  ['a', 'barber', 'kept', 'his', 'word', '.']\n",
      "sent_id:  [1, 2, 7, 5, 11, 0]\n",
      "sent   :  ['his', 'barber', 'kept', 'his', 'secret', '.']\n",
      "sent_id:  [5, 2, 7, 5, 3, 0]\n",
      "sent   :  ['but', 'keeping', 'and', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'the', 'barber', 'crazy', '.']\n",
      "sent_id:  [16, 12, 17, 12, 18, 1, 4, 3, 19, 20, 21, 22, 9, 2, 23, 0]\n",
      "sent   :  ['the', 'barber', 'went', 'up', 'a', 'huge', 'mountain', '.']\n",
      "sent_id:  [9, 2, 24, 25, 1, 4, 26, 0]\n"
     ]
    }
   ],
   "source": [
    "sentence_id = []\n",
    "\n",
    "for idx, sent in enumerate(sentences):\n",
    "    sent_id = []\n",
    "    for word in sent:\n",
    "        word_id = word_to_index[word]\n",
    "        sent_id.append(word_id)\n",
    "    print(\"sent   : \", sent)\n",
    "    print(\"sent_id: \", sent_id)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
